{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99ce69dc-3a55-4875-954a-e353d9be7e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explicacion del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d7ad2c0-bc2b-4d64-bac4-1cb2769f0ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "//EJEMPLO 1: MODELO DE CLASIFICACION CON SCALA\n",
    "\n",
    "// Databricks notebook source\n",
    "/////////////////////////////////////////////\n",
    "/// SIMPLE VERSION OF LOG REG EXAMPLE //////\n",
    "///////////////////////////////////////////\n",
    "\n",
    "// Note that usually all imports would occur at the top and\n",
    "// most of this would be in an object this layout if for learning purposes only\n",
    "\n",
    "// Logistic Regression Example\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "//LogisticRegression: La clase que provee Spark para entrenar modelos de regresión logística\n",
    "//SparkSession: Punto de entrada principal para trabajar con DataFrames y el stack de SQL en Spark.\n",
    "//Log4j: Se utiliza para configurar el nivel de logs de Spark (en este caso se setea a ERROR para evitar mensajes de advertencia extensos).\n",
    "\n",
    "// Spark Session\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Use Spark to read in the Titanic csv file.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"FileStore/tables/titanic.csv\")\n",
    "\n",
    "// Print the Schema of the DataFrame\n",
    "data.printSchema()\n",
    "\n",
    "///////////////////////\n",
    "/// Display Data /////\n",
    "/////////////////////\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}\n",
    "\n",
    "////////////////////////////////////////////////////\n",
    "//// Setting Up DataFrame for Machine Learning ////\n",
    "//////////////////////////////////////////////////\n",
    "\n",
    "// Grab only the columns we want\n",
    "val logregdataall = data.select(data(\"Survived\").as(\"label\"), $\"Pclass\", $\"Sex\", $\"Age\", $\"SibSp\", $\"Parch\", $\"Fare\", $\"Embarked\")\n",
    "val logregdata = logregdataall.na.drop()\n",
    "\n",
    "//Se seleccionan las columnas relevantes y se renombra Survived como label (columna que Spark ML interpretará como objetivo de clasificación).\n",
    "//Se eliminan las filas con valores nulos (na.drop()).\n",
    "\n",
    "\n",
    "// A few things we need to do before Spark can accept the data!\n",
    "// We need to deal with the Categorical columns\n",
    "\n",
    "// Import VectorAssembler and Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "// Deal with Categorical Columns\n",
    "\n",
    "val genderIndexer = new StringIndexer().setInputCol(\"Sex\").setOutputCol(\"SexIndex\")\n",
    "val embarkIndexer = new StringIndexer().setInputCol(\"Embarked\").setOutputCol(\"EmbarkIndex\")\n",
    "\n",
    "val genderEncoder = new OneHotEncoder().setInputCol(\"SexIndex\").setOutputCol(\"SexVec\")\n",
    "val embarkEncoder = new OneHotEncoder().setInputCol(\"EmbarkIndex\").setOutputCol(\"EmbarkVec\")\n",
    "\n",
    "// StringIndexer: Convierte valores categóricos (ej. \"male\", \"female\") en índices numéricos.\n",
    "//Sex → SexIndex\n",
    "//Embarked → EmbarkIndex\n",
    "//OneHotEncoder: Convierte el índice numérico en un vector binario donde cada posición representa una categoría.\n",
    "//SexIndex → SexVec\n",
    "//EmbarkIndex → EmbarkVec\n",
    "\n",
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                  .setInputCols(Array(\"Pclass\", \"SexVec\", \"Age\",\"SibSp\",\"Parch\",\"Fare\",\"EmbarkVec\"))\n",
    "                  .setOutputCol(\"features\") )\n",
    "\n",
    "//VectorAssembler: Toma columnas numéricas (incluyendo las transformaciones One-Hot) y las combina en un único vector llamado features. Este vector es lo que Spark ML utiliza para entrenar los modelos.\n",
    "\n",
    "////////////////////////////\n",
    "/// Split the Data ////////\n",
    "//////////////////////////\n",
    "val Array(training, test) = logregdata.randomSplit(Array(0.7, 0.3), seed = 12345)\n",
    "\n",
    "// Se divide el DataFrame en 70% para training y 30% para test. Se fija una semilla (seed = 12345) para mantener la reproducibilidad de la división.\n",
    "\n",
    "///////////////////////////////\n",
    "// Set Up the Pipeline ///////\n",
    "/////////////////////////////\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,assembler, lr))\n",
    "\n",
    "// Pipeline: Permite encadenar etapas de transformación y un estimador final (en este caso, la regresión logística).\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Ajusta (entrena) la pipeline con los datos de entrenamiento.\n",
    "// Devuelve un PipelineModel que internamente contiene los transformadores ajustados y el modelo de regresión logística entrenado.\n",
    "\n",
    "// Get Results on Test Set\n",
    "val results = model.transform(test)\n",
    "\n",
    "// Aplica el modelo entrenado sobre el conjunto de prueba.\n",
    "// Devuelve un DataFrame con nuevas columnas, incluyendo la predicción (prediction) para cada fila.\n",
    "\n",
    "////////////////////////////////////\n",
    "//// MODEL EVALUATION /////////////\n",
    "//////////////////////////////////\n",
    "\n",
    "// For Metrics and Evaluation\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "// Need to convert to RDD to use this\n",
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd\n",
    "\n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "\n",
    "// MulticlassMetrics: Clase de Spark para calcular métricas de evaluación (precision, recall, F1, etc.).\n",
    "// Se imprime la matriz de confusión para observar cuántas instancias han sido clasificadas correctamente o incorrectamente en cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802a6586-c728-4804-bd05-41b00f643fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "//EJEMPLO 2: MODELO DE REGRESION CON SCALA\n",
    "\n",
    "/ Databricks notebook source\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "// To see less warnings\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "// Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// RegressionEvaluator: Permite evaluar el rendimiento de modelos de regresión.\n",
    "// LinearRegression: Algoritmo de regresión lineal de Spark MLlib.\n",
    "// ParamGridBuilder y TrainValidationSplit: Herramientas para la validación y búsqueda de hiperparámetros (aunque no se usan directamente en este ejemplo, se importan).\n",
    "// Logger y Level de org.apache.log4j para configurar el nivel de log y así reducir la verbosidad de las salidas.\n",
    "//\tSparkSession: Punto de entrada principal de Spark.\n",
    "//\tVectorAssembler: Permite combinar múltiples columnas en un único vector de características \n",
    "//\tVectors: Provee estructuras de datos para representar vectores en Spark.\n",
    "\n",
    "// Prepare training and test data.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"FileStore/tables/Clean_USA_Housing.csv\")\n",
    "\n",
    "// Check out the Data\n",
    "data.printSchema()\n",
    "\n",
    "// See an example of what the data looks like\n",
    "// by printing out a Row\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}\n",
    "\n",
    "////////////////////////////////////////////////////\n",
    "//// Setting Up DataFrame for Machine Learning ////\n",
    "//////////////////////////////////////////////////\n",
    "\n",
    "// A few things we need to do before Spark can accept the data!\n",
    "// It needs to be in the form of two columns\n",
    "// (\"label\",\"features\")\n",
    "\n",
    "// This will allow us to join multiple feature columns\n",
    "// into a single column of an array of feautre values\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "// Rename Price to label column for naming convention.\n",
    "// Grab only numerical columns from the data\n",
    "val df = data.select(data(\"Price\").as(\"label\"),$\"Avg Area Income\",$\"Avg Area House Age\",$\"Avg Area Number of Rooms\",$\"Area Population\")\n",
    "\n",
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model\n",
    "\n",
    "// Set the input columns from which we are supposed to read the values\n",
    "// Set the name of the column where the vector will be stored\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Rooms\",\"Area Population\")).setOutputCol(\"features\")\n",
    "\n",
    "// Use the assembler to transform our DataFrame to the two columns\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")\n",
    "\n",
    "// Create a Linear Regression Model object\n",
    "val lr = new LinearRegression()\n",
    "\n",
    "// Fit the model to the data\n",
    "\n",
    "// Note: Later we will see why we should split\n",
    "// the data first, but for now we will fit to all the data.\n",
    "val lrModel = lr.fit(output)\n",
    "\n",
    "// Print the coefficients and intercept for linear regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "\n",
    "// Summarize the model over the training set and print out some metrics!\n",
    "// Explore this in the spark-shell for more methods to call\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
    "println(s\"objectiveHistory: ${trainingSummary.objectiveHistory.toList}\")\n",
    "\n",
    "trainingSummary.residuals.show()\n",
    "\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"MSE: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")\n",
    "\n",
    "// trainingSummary es un LinearRegressionTrainingSummary que contiene información sobre el proceso de entrenamiento.\n",
    "//\tnumIterations: número de iteraciones realizadas por el algoritmo de optimización.\n",
    "//\tobjectiveHistory: histórico de valores de la función objetivo durante las iteraciones.\n",
    "//\tresiduals: diferencia entre los valores predichos y los valores reales (etiquetados).\n",
    "//\tRMSE (Root Mean Squared Error): error cuadrático medio de la raíz, métrica muy común en regresión.\n",
    "//\tMSE (Mean Squared Error): error cuadrático medio.\n",
    "//\tr2 (R-squared): coeficiente de determinación, indica la proporción de la varianza explicada por el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ccb175-857b-4516-9ef4-2a57821eed30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "//EJEMPLO 3: GRIDSEARCH CON SCALA\n",
    "\n",
    "// Databricks notebook source\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "// RegressionEvaluator: Clase para evaluar el rendimiento de un modelo de regresión, normalmente usando métricas como RMSE, MSE, R-squared, etc.\n",
    "//\tLinearRegression: Algoritmo de regresión lineal provisto por Spark MLlib.\n",
    "//\tParamGridBuilder y TrainValidationSplit: Herramientas para la búsqueda sistemática de hiperparámetros y la validación del modelo.\n",
    "//\tLogger.getLogger(\"org\").setLevel(Level.ERROR): Ajusta el nivel de logging para reducir la cantidad de mensajes que aparecen en consola, facilitando la lectura de resultados.\n",
    "\n",
    "\n",
    "// Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Prepare training and test data.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"FileStore/tables/Clean_USA_Housing-1.csv\")\n",
    "data.printSchema()\n",
    "\n",
    "// See an example of what the data looks like\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}\n",
    "\n",
    "////////////////////////////////////////////////////\n",
    "//// Setting Up DataFrame for Machine Learning ////\n",
    "//////////////////////////////////////////////////\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "// Rename label column\n",
    "// Grab only numerical columns\n",
    "val df = data.select(data(\"Price\").as(\"label\"),$\"Avg Area Income\",$\"Avg Area House Age\",$\"Avg Area Number of Rooms\",$\"Area Population\")\n",
    "\n",
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model\n",
    "\n",
    "// Set the input columns from which we are supposed to read the values\n",
    "// Set the name of the column where the vector will be stored\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Rooms\",\"Area Population\")).setOutputCol(\"features\")\n",
    "\n",
    "// Transform the DataFrame\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")\n",
    "\n",
    "// Create an array of the training and test data\n",
    "val Array(training, test) = output.select(\"label\",\"features\").randomSplit(Array(0.7, 0.3), seed = 12345)\n",
    "\n",
    "//////////////////////////////////////\n",
    "//////// LINEAR REGRESSION //////////\n",
    "////////////////////////////////////\n",
    "val lr = new LinearRegression()\n",
    "\n",
    "//////////////////////////////////////\n",
    "/// PARAMETER GRID BUILDER //////////\n",
    "////////////////////////////////////\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam,Array(1000,0.001)).build()\n",
    "//model.validationMetrics\n",
    "\n",
    "// ParamGridBuilder permite construir una cuadrícula de parámetros.\n",
    "//\tAquí se está explorando el parámetro regParam (regularización) con dos valores: 1000 y 0.001.\n",
    "//\tbuild() finaliza la construcción de la cuadrícula de parámetros.\n",
    "\n",
    "///////////////////////\n",
    "// TRAIN TEST SPLIT //\n",
    "/////////////////////\n",
    "\n",
    "// In this case the estimator is simply the linear regression.\n",
    "// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// 80% of the data will be used for training and the remaining 20% for validation.\n",
    "val trainValidationSplit = (new TrainValidationSplit()\n",
    "                            .setEstimator(lr)\n",
    "                            .setEvaluator(new RegressionEvaluator) \n",
    "                            .setEstimatorParamMaps(paramGrid)\n",
    "                            .setTrainRatio(0.8) )\n",
    "\n",
    "\n",
    "// \tTrainValidationSplit:\n",
    "//\tsetEstimator: El modelo a entrenar, en este caso lr.\n",
    "//\tsetEvaluator: Evalúa el rendimiento del modelo, aquí con RegressionEvaluator por defecto (métricas como RMSE).\n",
    "//\tsetEstimatorParamMaps: La rejilla de hiperparámetros generada por ParamGridBuilder.\n",
    "//setTrainRatio(0.8): Define el 80% de los datos (del conjunto de entrenamiento) para entrenamiento interno y el 20% para validación interna.\n",
    "//En otras palabras, de la parte de training, Spark tomará el 80% para entrenar y el 20% para validar, eligiendo así la mejor configuración de hiperparámetros.\n",
    "\n",
    "// You can then treat this object as the new model and use fit on it.\n",
    "// Run train validation split, and choose the best set of parameters.\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "// \tAjusta (fit) el modelo seleccionando los mejores hiperparámetros del paramGrid.\n",
    "// El objeto resultante model es el mejor modelo encontrado (el que obtiene la mejor métrica definida por RegressionEvaluator).\n",
    "\n",
    "//////////////////////////////////////\n",
    "// EVALUATION USING THE TEST DATA ///\n",
    "////////////////////////////////////\n",
    "\n",
    "// Make predictions on test data. model is the model with combination of parameters\n",
    "// that performed best.\n",
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "//\ttransform(test) aplica el modelo final sobre los datos de prueba. Se muestran las columnas features, label (etiqueta real) y prediction (predicción del modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b9f352-c604-4375-8071-445bc798766c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// **********************************************\n",
    "// EJEMPLO 4: MODELO DE CLUSTERIZACION CON SCALA\n",
    "// **********************************************\n",
    "\n",
    "// IMPORTACIONES INICIALES\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "\n",
    "// 1. Lectura y Exploración de Datos\n",
    "val csv = spark.read\n",
    "  .option(\"inferSchema\",\"true\")  // Inferir tipos de columna\n",
    "  .option(\"header\", \"true\")      // Primera fila como encabezados\n",
    "  .csv(\"FileStore/tables/Mall_Customers-1.csv\")\n",
    "\n",
    "// Muestra las primeras filas del DataFrame\n",
    "csv.show()\n",
    "\n",
    "// Muestra el esquema de las columnas\n",
    "csv.printSchema()\n",
    "\n",
    "// Genera descripción estadística de algunas columnas\n",
    "csv.select(\"CustomerID\", \"Gender\", \"Age\", \"AnnualIncome\",\"SpendingScore\").describe().show()\n",
    "\n",
    "// Crea vista temporal para consultas SQL\n",
    "csv.createOrReplaceTempView(\"MallCustomerData\")\n",
    "\n",
    "// 2. Visualización con SQL (opcional dentro de Databricks)\n",
    "// %sql\n",
    "// select * from MallCustomerData\n",
    "\n",
    "// 3. División de Datos en Entrenamiento y Prueba\n",
    "val Array(train, test) = csv.randomSplit(Array(0.7, 0.3), seed=1234) \n",
    "// Se añade un seed para reproducibilidad\n",
    "\n",
    "// Conteo de filas\n",
    "val trainRows = train.count()\n",
    "val testRows = test.count()\n",
    "println(s\"Training Rows: $trainRows, Testing Rows: $testRows\")\n",
    "\n",
    "// 4. Ensamblado de Características\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"AnnualIncome\", \"SpendingScore\")) // Columnas importantes para K-means\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "// Transformación del conjunto de entrenamiento\n",
    "val training = assembler.transform(train)\n",
    "training.show(5) // Muestra las primeras 5 filas transformadas\n",
    "\n",
    "// 5. Entrenamiento del Modelo K-means\n",
    "val kmeans = new KMeans()\n",
    "  .setK(5)                           // Definimos 5 clusters\n",
    "  .setFeaturesCol(\"features\")        // Vector de características\n",
    "  .setPredictionCol(\"prediction\")    // Columna de resultado\n",
    "\n",
    "val kmeansModel = kmeans.fit(training)\n",
    "\n",
    "// 6. Transformación del Conjunto de Prueba\n",
    "val testing = assembler.transform(test)\n",
    "testing.show(5)\n",
    "\n",
    "// 7. Predicción en el Conjunto de Prueba\n",
    "val prediction = kmeansModel.transform(testing)\n",
    "prediction.show(10)\n",
    "\n",
    "// 8. Análisis de Resultados\n",
    "prediction.groupBy(\"prediction\").count().show()\n",
    "\n",
    "// Crear vista temporal de los resultados\n",
    "prediction.createOrReplaceTempView(\"CustomerClusterMallData\")\n",
    "\n",
    "// 9. Consulta Final (SQL) de la Asignación de Clusters\n",
    "// %sql\n",
    "// select AnnualIncome, SpendingScore, prediction from CustomerClusterMallData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ac8562-0890-4dc4-ba59-487f3e81fd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// EJEMPLO 5: MODELO DE PCA CON SCALA\n",
    "\n",
    "// Databricks notebook source\n",
    "// DBTITLE 1,ChiSqSelector\n",
    "\n",
    "//PCA\n",
    "\n",
    "import org.apache.spark.ml.feature.PCA\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "// org.apache.spark.ml.feature.PCA: Proporciona la clase PCA que se utiliza para entrenar y aplicar el análisis de componentes principales.\n",
    "// org.apache.spark.ml.linalg.Vectors: Permite crear objetos de tipo Vector (tanto densos como dispersos) que son necesarios para alimentar a las transformaciones y modelos en Spark ML.\n",
    "\n",
    "\n",
    "val data = Array(\n",
    "  Vectors.sparse(5, Seq((1,1.0),(3,7.0))),\n",
    "  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "  Vectors.dense(4.0, 0.0, 3.0, 6.0, 7.0)\n",
    ")\n",
    "\n",
    "val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "df.show(false)\n",
    "\n",
    "// data.map(Tuple1.apply) convierte cada vector en una tupla de un solo elemento (requerido para que Spark pueda crear el DataFrame de manera adecuada).\n",
    "// spark.createDataFrame(...) crea el DataFrame a partir de dichas tuplas.\n",
    "// .toDF(\"features\") asigna el nombre features a la columna que contiene los vectores.\n",
    "// df.show(false) muestra el DataFrame sin truncar el contenido de las filas, para poder observar los vectores completos.\n",
    "// El resultado es un DataFrame con una sola columna denominada features. Cada fila corresponde a uno de los vectores definidos.\n",
    "\n",
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(3)\n",
    "  .fit(df)\n",
    "\n",
    "//1.\tnew PCA(): Crea una instancia del modelo PCA.\n",
    "//2.\t.setInputCol(\"features\"): Especifica la columna de entrada que contiene los vectores originales.\n",
    "//3.\t.setOutputCol(\"pcaFeatures\"): Nombra la columna de salida que contendrá el resultado de la transformación PCA.\n",
    "//4.\t.setK(3): Establece que se extraerán 3 componentes principales.\n",
    "//5.\t.fit(df): Ajusta el modelo a los datos del DataFrame df. Internamente, PCA calcula los autovectores y autovalores de la matriz de covarianza derivada de la columna features.\n",
    "\n",
    "\n",
    "val result = pca.transform(df).select(\"pcaFeatures\")\n",
    "\n",
    "result.show(false)\n",
    "\n",
    "//\tpca.transform(df): Aplica la transformación PCA sobre la columna features. El resultado es un DataFrame que contiene la columna original más la nueva columna pcaFeatures.\n",
    "//\t.select(\"pcaFeatures\"): Selecciona únicamente la columna pcaFeatures para mostrarla.\n",
    "//\tresult.show(false): Muestra el contenido sin truncar.\n",
    "// Aquí es donde observamos las nuevas representaciones de cada vector en el espacio de 3 dimensiones (los tres componentes principales).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Entregables Big Data Advanced - Agurto,Camila",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
